# Import the pandas library again (this line is redundant and can be omitted)
import pandas as pd
import nltk
import re
# Import necessary modules from the NLTK library for text processing
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
# Import CountVectorizer from scikit-learn for text vectorization
from sklearn.feature_extraction.text import CountVectorizer
# Import train_test_split from scikit-learn for splitting data into training and test sets
from sklearn.model_selection import train_test_split
# Import CountVectorizer again (redundant import)
from sklearn.feature_extraction.text import CountVectorizer
# Import cosine_similarity from scikit-learn for computing similarity between vectors
from sklearn.metrics.pairwise import cosine_similarity
# Import the os module for interacting with the operating system
import os
# Import the pickle module for serializing Python objects
import pickle

# Load a dataset from a CSV file into a pandas DataFrame
movies = pd.read_csv('movies.csv')

# Display the first 10 rows of the DataFrame
movies.head(10)

# Generate descriptive statistics of the DataFrame
movies.describe()

# Print a concise summary of the DataFrame (information about columns, data types, non-null values, etc.)
movies.info()

# Calculate the sum of null (missing) values for each column in the DataFrame
movies.isnull().sum()

# Get the names of columns in the DataFrame
movies.columns

# Select and retain only specified columns ('id', 'title', 'overview', 'genre') from the DataFrame
movies = movies[['id', 'title', 'overview', 'genre']]

# Add a new column 'tags' by concatenating 'overview' and 'genre' columns
movies['tags'] = movies['overview'] + movies['genre']

# Create a new DataFrame 'new_data' by dropping the 'overview' and 'genre' columns from 'movies'
new_data = movies.drop(columns=['overview', 'genre'])

# Download NLTK resources for tokenization, lemmatization, and stopwords
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# Define a function for cleaning text data
def clean_text(text):
    # Return an empty string if text is not a string
    if not isinstance(text, str):
        return ""
    # Convert text to lowercase
    text = text.lower()
    # Remove punctuation while retaining words and digits
    text = re.sub(r'[^\\w\\s\\d]', '', text)
    # Tokenize the text into words
    words = word_tokenize(text)
    # Define English stopwords
    stop_words = set(stopwords.words('english'))
    # Remove stopwords from the tokenized words
    words = [word for word in words if word not in stop_words]
    # Initialize the WordNet lemmatizer
    lemmatizer = WordNetLemmatizer()
    # Lemmatize each word
    words = [lemmatizer.lemmatize(word) for word in words]
    # Join the words back into a single string
    text = ' '.join(words)
    return text

# Apply the clean_text function to the 'tags' column of 'new_data' and store the result in 'tags_clean'
new_data['tags_clean'] = new_data['tags'].apply(clean_text)

# Reapply the clean_text function to the 'tags' column (this line seems redundant)
new_data['tags_clean'] = new_data['tags'].apply(clean_text)

# Initialize a CountVectorizer object with a maximum of 10,000 features and English stop words
cv = CountVectorizer(max_features=10000, stop_words='english')

# Fit the CountVectorizer to the 'tags_clean' column and transform the text data into a numerical vector representation
vector = cv.fit_transform(new_data['tags_clean'].values.astype('U')).toarray()

# Check the shape of the resulting vector
vector.shape

# Calculate the cosine similarity between vectors
similarity = cosine_similarity(vector)

# Print a concise summary of the 'new_data' DataFrame
new_data.info()

# Calculate similarity scores for the third movie with all other movies, sort them, and store the result
distance = sorted(list(enumerate(similarity[2])), reverse=True, key=lambda vector: vector[1])

# Print the titles of the first five movies most similar to the third movie
for i in distance[0:5]:
    print(new_data.iloc[i[0]].title)

# Define a function to recommend the top 5 similar movies for a given movie title
def recommend(movies):
    # Find the index of the given movie in the DataFrame
    index = new_data[new_data['title'] == movies].index[0]
    # Calculate similarity scores, sort them, and print titles of the top 5 similar movies
    distance = sorted(list(enumerate(similarity[index])), reverse=True, key=lambda vector: vector[1])
    for i in distance[0:5]:
        print(new_data.iloc[i[0]].title)

# Call the recommend function with "Iron Man" as the argument
recommend("Spirited Away")

# Serialize the 'new_data' DataFrame and save it to a file
pickle.dump(new_data, open('movies_list.pkl', 'wb'))
pickle.dump(new_data, open('similarity.pkl', 'wb'))

# Deserialize the 'movies_list.pkl' file back into a Python object
pickle.load(open('movies_list.pkl', 'rb'))

# Print the current working directory
print(os.getcwd())